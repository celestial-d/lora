# Federated Instruction Tuning on General Dataset
---

num_clients: 2 # total number of clients
num_rounds: 100

dataset:
  name: "lucasmccabe-lmi/CodeAlpaca-20k"
#timdettmers/openassistant-guanaco
#stanfordnlp/imdb 
#lucasmccabe-lmi/CodeAlpaca-20k
#facebook/opt-1.3b
#facebook/opt-2.7b
#facebook/opt-6.7b
#facebook/opt-13b
#facebook/opt-30b
model:
  name: "facebook/opt-350m"
  quantization: 8 # 8 or 4 if you want to do quantization with BitsAndBytes
  cut_layer: 2
  gradient_checkpointing: True
  transformer_module_name: transformer.h
  layers_after_transformer:
    - transformer.ln_f
    - lm_head
  lora:
    peft_lora_r: 8
    peft_lora_alpha: 64

train:
  num_rounds: ${num_rounds}
  save_every_round: 5
  learning_rate_max: 5e-5
  learning_rate_min: 1e-6
  seq_length: 128
  training_arguments:
    output_dir: null # to be set by hydra
    learning_rate: null # to be set by the client
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 1
    logging_steps: 10
    num_train_epochs: 1
    max_steps: 10
    report_to: null
    save_steps: 1000
    save_total_limit: 10
    gradient_checkpointing: ${model.gradient_checkpointing}
    lr_scheduler_type: "constant"

strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.1 # sample 10% of clients (i.e. 2 per round)
  fraction_evaluate: 0.0 # no client evaluation

#client_resources:
#  num_cpus: 8
#  num_gpus: 1.0
